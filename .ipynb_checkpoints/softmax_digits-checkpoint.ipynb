{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-notice",
   "metadata": {},
   "source": [
    "# Softmax regression for handwritten digits\n",
    "\n",
    "Today, we'll implement a softmax classifier recognizing handwritten digits.  We'll begin by using a relatively small collection (around 1800) of low resolution (8 by 8pix) digits.  This can be easily acquired using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "international-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea4d5343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# num images, num pixels\n",
    "# digits.data.shape\n",
    "plt.imshow(digits.data[0].reshape(8,8))\n",
    "print(digits.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8d18faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-gnome",
   "metadata": {},
   "source": [
    "The digits appear as an $m\\times n$ array, where $m$ is the number of data instances and $n$ is the number of features.  It's important to recognize that for this problem, the number of features is $8\\times8 = 64$: the instances are flattened.  If you want to plot a digit from the dataset using, for example, matplotlib's imshow, you'll need to reshape this.  \n",
    "\n",
    "You'll also want to be careful to normalize the data, preferably by subtracting the mean and dividing by the standard deviation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "rotary-whole",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8117562 , -0.8117562 ,  0.01925204, ..., -0.8117562 ,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [-0.8117562 , -0.8117562 , -0.8117562 , ...,  0.85026027,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [-0.8117562 , -0.8117562 , -0.8117562 , ...,  1.84747015,\n",
       "         0.68405863, -0.8117562 ],\n",
       "       ...,\n",
       "       [-0.8117562 , -0.8117562 , -0.64555455, ...,  0.18545368,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [-0.8117562 , -0.8117562 , -0.4793529 , ...,  1.18266357,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [-0.8117562 , -0.8117562 ,  0.85026027, ...,  1.18266357,\n",
       "        -0.64555455, -0.8117562 ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Perform normalization\n",
    "X = digits.data\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-anthony",
   "metadata": {},
   "source": [
    "The labels appear as integers.  Write and apply a function that converts from this integer representation to a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "vietnamese-thesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Convert the labels to a one-hot encoding\n",
    "def one_hot(y):\n",
    "    N = len(np.unique(y)) # The number of classes (different digits)\n",
    "    m = len(y) # The number of data points (images)\n",
    "    z = np.zeros((m,N))\n",
    "    for i in range(m):\n",
    "        z[i,y[i]] = 1\n",
    "    return z\n",
    "\n",
    "z = one_hot(digits.target)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-wellington",
   "metadata": {},
   "source": [
    "Another important step is to split the dataset into training and testing sets.  I like using the function sklearn.model_selection.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "systematic-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Split the dataset into training and testing sets\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "x_train,x_test,z_train,z_test = ms.train_test_split(X,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-climate",
   "metadata": {},
   "source": [
    "With data in hand, we now need to implement the model.  Recall that our predictions will be computed as\n",
    "$$\n",
    "Y_{pred} = \\mathrm{Softmax}(\\Phi W)\n",
    "$$\n",
    "Implement the softmax method, generate the matrix $\\Phi$ (I suggest a linear model, which is to say that all you need to do will be to prepend a column of ones to the $m\\times n$ matrix of pixel values, and instantiate the parameter matrix $W$ (I suggest instantiating to an array of very small random numbers).  Your implementation of Softmax should be vectorized, in that it should take a $m \\times N$ array of logits and output and $m \\times N$ array without using a loop.  Make a prediction using this untrained model: a sensible result at this stage is that all classes are approximately equally likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "rental-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of softmax\n",
    "import numpy as np\n",
    "def Softmax(a):\n",
    "    numerator = np.exp(a)\n",
    "    denominator = numerator.sum(axis=1)\n",
    "    return numerator/denominator[:, np.newaxis]\n",
    "\n",
    "# a = np.array([[1.,2.,3.], [2.,3.,4.]])\n",
    "# Softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2822da93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.8117562 , -0.8117562 , ..., -0.8117562 ,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [ 1.        , -0.8117562 , -0.8117562 , ...,  0.85026027,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [ 1.        , -0.8117562 , -0.8117562 , ...,  1.84747015,\n",
       "         0.68405863, -0.8117562 ],\n",
       "       ...,\n",
       "       [ 1.        , -0.8117562 , -0.8117562 , ...,  0.18545368,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [ 1.        , -0.8117562 , -0.8117562 , ...,  1.18266357,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [ 1.        , -0.8117562 , -0.8117562 , ...,  1.18266357,\n",
       "        -0.64555455, -0.8117562 ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Phi\n",
    "# My attempt was the same as np.r_\n",
    "# Phi = np.insert(X, 0, np.ones_like(X[:,0]), axis=0) \n",
    "Phi = np.c_[np.ones_like(X[:,0]),X]\n",
    "# Phi[:,0]\n",
    "Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf7cf121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build W\n",
    "N = len(digits.target_names) # The number of classes (different digits)\n",
    "m = X.shape[0]   # The number of data points (images)\n",
    "n = X.shape[1] + 1   # The number of features (pixels), add one for column of ones\n",
    "\n",
    "W = np.random.randn(n,N)\n",
    "#W\n",
    "\n",
    "a = Phi@W\n",
    "\n",
    "Softmax(a).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-effectiveness",
   "metadata": {},
   "source": [
    "Now generate functions (or one function with multiple outputs) to compute the categorical cross entropy and its gradient.  These are given by \n",
    "$$\n",
    "\\mathcal{L}(W,\\Phi,Y_{obs}) = -\\frac{1}{mN} \\sum_{i=1}^m \\left(Y_{obs,i} \\cdot \\ln \\mathrm{Softmax}(\\Phi_i W)\\right).\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{ \\partial \\mathcal{L}}{\\partial W} = -\\frac{1}{mN} \\sum_{i=1}^m \\left[(Y_{obs,i} - \\mathrm{Softmax}(\\Phi W)_i)^T \\Phi_i \\right]^T. \n",
    "$$\n",
    "As you implement these functions, consider how to do so in as efficient a manner as possible.  Note that it is possible to vectorize the sums.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-czech",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "selective-minority",
   "metadata": {},
   "source": [
    "Implement gradient descent and train this model.  Record the value of $\\mathcal{L}$ as a function of gradient descent iteration, and produce a plot convincing yourself that the model is converging to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-filing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "static-albert",
   "metadata": {},
   "source": [
    "One very interesting result of working with image data is that we can interpret the learned parameters as images (the weight matrix is $N\\times (1+n)$.  If you get rid of the first entry, which corresponds to a constant offset, the remaining $N \\times n$ weights are each associated with a given input pixel for a given class).  Plot your weights as images (there should be ten of them).  Evaluate the pattern that you find.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-adventure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-allergy",
   "metadata": {},
   "source": [
    "Finally, once this task is complete, scale your method up to the larger (in both number of instances and resolution) dataset MNIST (you can get it using the command sklearn.datasets.fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)).  This will take substantial time to train!  Only do this once you are satisfied with your implementation on the digits dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-jones",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
