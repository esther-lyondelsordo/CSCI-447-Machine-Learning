{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-notice",
   "metadata": {},
   "source": [
    "# Softmax regression for handwritten digits\n",
    "\n",
    "Today, we'll implement a softmax classifier recognizing handwritten digits.  We'll begin by using a relatively small collection (around 1800) of low resolution (8 by 8pix) digits.  This can be easily acquired using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# num images, num pixels\n",
    "# digits.data.shape\n",
    "plt.imshow(digits.data[0].reshape(8,8))\n",
    "print(digits.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d18faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-gnome",
   "metadata": {},
   "source": [
    "The digits appear as an $m\\times n$ array, where $m$ is the number of data instances and $n$ is the number of features.  It's important to recognize that for this problem, the number of features is $8\\times8 = 64$: the instances are flattened.  If you want to plot a digit from the dataset using, for example, matplotlib's imshow, you'll need to reshape this.  \n",
    "\n",
    "You'll also want to be careful to normalize the data, preferably by subtracting the mean and dividing by the standard deviation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Perform normalization\n",
    "X = digits.data\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-anthony",
   "metadata": {},
   "source": [
    "The labels appear as integers.  Write and apply a function that converts from this integer representation to a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Convert the labels to a one-hot encoding\n",
    "def one_hot(y):\n",
    "    N = len(np.unique(y)) # The number of classes (different digits)\n",
    "    m = len(y) # The number of data points (images)\n",
    "    z = np.zeros((m,N))\n",
    "    for i in range(m):\n",
    "        z[i,y[i]] = 1\n",
    "    return z\n",
    "\n",
    "z = one_hot(digits.target)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-wellington",
   "metadata": {},
   "source": [
    "Another important step is to split the dataset into training and testing sets.  I like using the function sklearn.model_selection.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Split the dataset into training and testing sets\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "x_train,x_test,z_train,z_test = ms.train_test_split(X,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-climate",
   "metadata": {},
   "source": [
    "With data in hand, we now need to implement the model.  Recall that our predictions will be computed as\n",
    "$$\n",
    "Y_{pred} = \\mathrm{Softmax}(\\Phi W)\n",
    "$$\n",
    "Implement the softmax method, generate the matrix $\\Phi$ (I suggest a linear model, which is to say that all you need to do will be to prepend a column of ones to the $m\\times n$ matrix of pixel values, and instantiate the parameter matrix $W$ (I suggest instantiating to an array of very small random numbers).  Your implementation of Softmax should be vectorized, in that it should take a $m \\times N$ array of logits and output and $m \\times N$ array without using a loop.  Make a prediction using this untrained model: a sensible result at this stage is that all classes are approximately equally likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of softmax\n",
    "def Softmax(a):\n",
    "    numerator = np.exp(a)\n",
    "    denominator = numerator.sum(axis=1)\n",
    "    return numerator/denominator[:, np.newaxis]\n",
    "\n",
    "# a = np.array([[1.,2.,3.], [2.,3.,4.]])\n",
    "# Softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Phi\n",
    "# My attempt was the same as np.r_\n",
    "# Phi = np.insert(X, 0, np.ones_like(X[:,0]), axis=0) \n",
    "Phi = np.c_[np.ones_like(x_train[:,0]),x_train]\n",
    "# Phi[:,0]\n",
    "Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7cf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build W\n",
    "N = len(digits.target_names) # The number of classes (different digits)\n",
    "m = X.shape[0]   # The number of data points (images)\n",
    "n = X.shape[1] + 1   # The number of features (pixels), add one for column of ones\n",
    "\n",
    "W = np.random.randn(n,N)\n",
    "W = W / 10000 # divide to reduce noise\n",
    "\n",
    "a = Phi@W\n",
    "\n",
    "Softmax(a).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-effectiveness",
   "metadata": {},
   "source": [
    "Now generate functions (or one function with multiple outputs) to compute the categorical cross entropy and its gradient.  These are given by \n",
    "$$\n",
    "\\mathcal{L}(W,\\Phi,Y_{obs}) = -\\frac{1}{mN} \\sum_{i=1}^m \\sum_{j=1}^N \\left(Y_{obs,i} \\cdot \\ln \\mathrm{Softmax}(\\Phi_i W)\\right).\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{ \\partial \\mathcal{L}}{\\partial W} = -\\frac{1}{mN} \\sum_{i=1}^m \\left[(Y_{obs,i} - \\mathrm{Softmax}(\\Phi W)_i)^T \\Phi_i \\right]^T. \n",
    "$$\n",
    "As you implement these functions, consider how to do so in as efficient a manner as possible.  Note that it is possible to vectorize the sums.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(Y,Phi,W):\n",
    "    m = len(Y)\n",
    "    N = len(Y[0])\n",
    "    a = Phi@W\n",
    "    L_ = -(1/(m*N))*Y*np.log(Softmax(a))\n",
    "    return L_.sum()\n",
    "\n",
    "def L_grad(Y,Phi,W):\n",
    "    m = len(Y)\n",
    "    N = len(Y[0])\n",
    "    n = Phi.shape[1]\n",
    "    a = Phi@W\n",
    "    L_grad_ = np.zeros((n,N))\n",
    "    for _ in range(m):\n",
    "        L_grad_ += ((Y-Softmax(a)).T@Phi).T\n",
    "    return -(1/(m*N))*L_grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642591ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "L(z_train,Phi,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_grad(z_train,Phi,W).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-minority",
   "metadata": {},
   "source": [
    "Implement gradient descent and train this model.  Record the value of $\\mathcal{L}$ as a function of gradient descent iteration, and produce a plot convincing yourself that the model is converging to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick an eeta\n",
    "eeta = 0.02\n",
    "n_samples = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent to train the W's\n",
    "w_vec = [W]\n",
    "for i in range(1, n_samples):\n",
    "    W = W - eeta*L_grad(z_train,Phi,W)\n",
    "    w_vec.append(W)\n",
    "    \n",
    "params = np.array(w_vec)\n",
    "\n",
    "\n",
    "# use trained W's to get Losses for test data at each step\n",
    "Phi = np.c_[np.ones_like(x_test[:,0]),x_test]\n",
    "Loss = L(z_test,Phi,W)\n",
    "L_vec = [Loss]\n",
    "for i in range(1, n_samples):\n",
    "    Loss1 = L(z_test,Phi,params[i])\n",
    "    L_vec.append(Loss1)\n",
    "    \n",
    "Losses = np.array(L_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160470ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params.shape)\n",
    "print(Losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9eb4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66437944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Loss at each step, should converge to zero\n",
    "x_axis = np.linspace(-5,5,n_samples)\n",
    "plt.plot(x_axis, Losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-albert",
   "metadata": {},
   "source": [
    "One very interesting result of working with image data is that we can interpret the learned parameters as images (the weight matrix is $N\\times (1+n)$.  If you get rid of the first entry, which corresponds to a constant offset, the remaining $N \\times n$ weights are each associated with a given input pixel for a given class).  Plot your weights as images (there should be ten of them).  Evaluate the pattern that you find.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(digits.data[0].reshape(8,8))\n",
    "# use params[-1] to get last W (learned parameters)\n",
    "data = params[-1][1:params[-1].shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3657c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5)\n",
    "axs[0,0].imshow(data[:,0].reshape(8,8))\n",
    "axs[0,1].imshow(data[:,1].reshape(8,8))\n",
    "axs[0,2].imshow(data[:,2].reshape(8,8))\n",
    "axs[0,3].imshow(data[:,3].reshape(8,8))\n",
    "axs[0,4].imshow(data[:,4].reshape(8,8))\n",
    "axs[1,0].imshow(data[:,5].reshape(8,8))\n",
    "axs[1,1].imshow(data[:,6].reshape(8,8))\n",
    "axs[1,2].imshow(data[:,7].reshape(8,8))\n",
    "axs[1,3].imshow(data[:,8].reshape(8,8))\n",
    "axs[1,4].imshow(data[:,9].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ac876",
   "metadata": {},
   "source": [
    "**Pattern:** I see numbers 0 through 9!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-allergy",
   "metadata": {},
   "source": [
    "Finally, once this task is complete, scale your method up to the larger (in both number of instances and resolution) dataset MNIST (you can get it using the command sklearn.datasets.fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)).  This will take substantial time to train!  Only do this once you are satisfied with your implementation on the digits dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beautiful-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection as ms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19458c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "digits2 = sklearn.datasets.fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80cf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = digits2[0]\n",
    "labels = digits2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa498065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Perform normalization\n",
    "X = data\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e2bd258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Convert the labels to a one-hot encoding\n",
    "def one_hot(y):\n",
    "    N = len(np.unique(y)) # The number of classes (different digits)\n",
    "    m = len(y) # The number of data points (images)\n",
    "    z = np.zeros((m,N))\n",
    "    for i in range(m):\n",
    "        z[i,int(y[i])] = 1\n",
    "    return z\n",
    "\n",
    "z = one_hot(labels)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9de2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Split the dataset into training and testing sets\n",
    "x_train,x_test,z_train,z_test = ms.train_test_split(X,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493fd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Phis\n",
    "Phi_train = np.c_[np.ones_like(x_train[:,0]),x_train]\n",
    "Phi_test = np.c_[np.ones_like(x_test[:,0]),x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3d4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of softmax\n",
    "def Softmax(a):\n",
    "    numerator = np.exp(a)\n",
    "    denominator = numerator.sum(axis=1)\n",
    "    return numerator/denominator[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8698d9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build W\n",
    "N = len(np.unique(labels)) # The number of classes (different digits)\n",
    "m = X.shape[0]   # The number of data points (images)\n",
    "n = X.shape[1] + 1   # The number of features (pixels), add one for column of ones\n",
    "\n",
    "W = np.random.randn(n,N)\n",
    "W = W / 10000 # divide to reduce noise\n",
    "\n",
    "a = Phi_train@W\n",
    "\n",
    "Softmax(a).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "712c1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and gradient\n",
    "def L(Y,Phi,W):\n",
    "    m = len(Y)\n",
    "    N = len(Y[0])\n",
    "    a = Phi@W\n",
    "    L_ = -(1/(m*N))*Y*np.log(Softmax(a))\n",
    "    return L_.sum()\n",
    "\n",
    "def L_grad(Y,Phi,W):\n",
    "    m = len(Y)\n",
    "    N = len(Y[0])\n",
    "    n = Phi.shape[1]\n",
    "    a = Phi@W\n",
    "    L_grad_ = np.zeros((n,N))\n",
    "    for _ in range(m):\n",
    "        L_grad_ += ((Y-Softmax(a)).T@Phi).T\n",
    "    return -(1/(m*N))*L_grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eed77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick an eeta\n",
    "eeta = 100\n",
    "n_samples = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_grad(z_train,Phi_train,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent to train the W's\n",
    "w_vec = [W]\n",
    "for i in range(1, n_samples):\n",
    "    W = W - eeta*L_grad(z_train,Phi_train,W)\n",
    "    w_vec.append(W)\n",
    "    print(np.linalg.norm(W))\n",
    "    \n",
    "params = np.array(w_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb19925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trained W's to get Losses for test data at each step\n",
    "Loss = L(z_test,Phi_test,W)\n",
    "L_vec = [Loss]\n",
    "for i in range(1, n_samples):\n",
    "    Loss1 = L(z_test,Phi_test,params[i])\n",
    "    print(Loss1)\n",
    "    L_vec.append(Loss1)\n",
    "    \n",
    "Losses = np.array(L_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Loss at each step, should converge to zero\n",
    "x_axis = np.linspace(-5,5,n_samples)\n",
    "plt.plot(x_axis, Losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the new data from our learned parameters\n",
    "data = params[-1][1:params[-1].shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763938f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learned digit images\n",
    "fig, axs = plt.subplots(2,5)\n",
    "axs[0,0].imshow(data[:,0].reshape(28,28))\n",
    "axs[0,1].imshow(data[:,1].reshape(28,28))\n",
    "axs[0,2].imshow(data[:,2].reshape(28,28))\n",
    "axs[0,3].imshow(data[:,3].reshape(28,28))\n",
    "axs[0,4].imshow(data[:,4].reshape(28,28))\n",
    "axs[1,0].imshow(data[:,5].reshape(28,28))\n",
    "axs[1,1].imshow(data[:,6].reshape(28,28))\n",
    "axs[1,2].imshow(data[:,7].reshape(28,28))\n",
    "axs[1,3].imshow(data[:,8].reshape(28,28))\n",
    "axs[1,4].imshow(data[:,9].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92190f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
