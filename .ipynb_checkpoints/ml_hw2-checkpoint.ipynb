{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b624b3f9",
   "metadata": {},
   "source": [
    "<h1>Machine Learning Homework #2</h1>\n",
    "\n",
    "**Esther Lyon Delsordo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d16d2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713fef9",
   "metadata": {},
   "source": [
    "<h2>Problem 1: Full Bayesian Inference</h2>\n",
    "We have seen that we can estimate the maximum likelihood estimators for a variety of distributions from\n",
    "some observations. However it is also possible to infer full posterior distributions, rather than just point\n",
    "estimators for some useful cases.\n",
    "Coin flips\n",
    "Consider first the problem of trying to infer the weighted-ness $\\theta$ of a coin. In particular, use Bayes’ theorem\n",
    "to show that the posterior distribution\n",
    "$$P(\\theta|X)$$\n",
    "is Beta-distributed, assuming that the likelihood \n",
    "$$P(X|\\theta) = \\prod^m_{i=1} Bernoulli(X_i; \\theta)$$ \n",
    "is independent and Bernoulli distributed and the prior $P(\\theta) = Beta(\\theta; α, β)$ is Beta-distributed with known hyperparameters $α$ and $β$. \n",
    "(If you don’t know where to start with this: first, look up the PDF for a beta distribution, which you should use\n",
    "for the prior. Multiply it with the likelihood function. Collect terms and perhaps rename these collections\n",
    "so that the resulting posterior again has the PDF of a beta distribution).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea4724",
   "metadata": {},
   "source": [
    "**Ans:** The beta distribution, \n",
    "$$\n",
    "Beta(a,b) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n",
    "$$\n",
    "where B is a normalization function that is constant with respect to $\\theta$, can be used for our prior, $P(\\theta)$. By Bayes Theorem we can write:\n",
    "$$\n",
    "P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}\n",
    "$$\n",
    "Because $P(X)$ is also constant with respect to $\\theta$, we can say:\n",
    "$$\n",
    "\\frac{P(X|\\theta)P(\\theta)}{P(X)} \\propto P(X|\\theta)P(\\theta)\n",
    "$$\n",
    "We need to find our posterior now. For the Bernoulli distribution we have:\n",
    "$$\n",
    "P(X|\\theta) \\propto \\theta^z (1-\\theta)^{m-z}\n",
    "$$\n",
    "where $z = \\sum^m_{i=1}x_i$. We can use the Beta distribution for our prior, ignoring the denominator as below:\n",
    "$$\n",
    "P(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\n",
    "$$\n",
    "Then, with this information we have:\n",
    "$$\n",
    "P(\\theta|X) \\propto \\theta^{a+z-1}(1-\\theta)^{m+b-z-1}\n",
    "$$\n",
    "If we name two new variables, $a' = a + z$ and $b'=m+b-z$, we get a result that is in the same format as the Beta distribution:\n",
    "$$\n",
    "\\therefore P(\\theta|X) = \\frac{\\theta^{a'-1}(1-\\theta)^{b'-1}}{B(a',b')}\n",
    "$$\n",
    "Showing that the Bernoulli posterior distribution is also beta distributed. (The beta distribution is a conjugate prior for the Bernoulli distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04bddb",
   "metadata": {},
   "source": [
    "<h2>Problem 2: Feature augmentation</h2>\n",
    "<h3>(a)</h3>\n",
    "Suppose that you have a dataset that looks like Figure 1, and you would to perform linear regression on it.\n",
    "Describe your approach for constructing a design matrix $\\Phi$ for this problem. Specifically, what would you place in each of the columns of $\\Phi$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964b886b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Fig2_2a.png\" width=\"600\" alt=\"Figure 2a\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(alt=\"Figure 2a\", url=\"Fig2_2a.png\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad5b40",
   "metadata": {},
   "source": [
    "Design matrix with x, x^2, x^3??? Need to experiment more with this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa51a47",
   "metadata": {},
   "source": [
    "<h3>(b)</h3> Same question as above. Would the same approach that you used in Part 1, also work here? Why or why\n",
    "not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055d6bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Fig2_2b.png\" width=\"600\" alt=\"Figure 2b\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(alt=\"Figure 2b\", url=\"Fig2_2b.png\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fc9cb",
   "metadata": {},
   "source": [
    "No, the approach from before would not be able to account for that many maxima and minima in an efficient way. A sinusoidal function as one of the design matrix parameters would be an simpler way to fit this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc778c",
   "metadata": {},
   "source": [
    "<h2>Problem 3: Gradient Descent</h2>\n",
    "<h3>(a)</h3>\n",
    "Consider the function\n",
    "$$\\mathcal{L}(\\vec \\theta) = \\frac{1}{2}(\\theta_1^2 - \\theta_2)^2 + \\frac{1}{2}(\\theta_1 - 1)^2$$\n",
    "Derive an expression for the gradient of this function with respect to ~θ. Implement some code that uses\n",
    "gradient descent to find its minimum value. Create a plot similar to those we did in class that illustrates\n",
    "your algorithm’s progress to this minimum beginning from at least 3 randomly selected initial positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f082e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1b0742c",
   "metadata": {},
   "source": [
    "<h3>(c)</h3>\n",
    "Describe, as specifically as you can, why it is not possible to solve directly for the minimum values of the\n",
    "functions that you worked with above. Describe how this relates to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be9b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
