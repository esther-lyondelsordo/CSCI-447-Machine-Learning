{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-bikini",
   "metadata": {},
   "source": [
    "# Regularization in neural nets\n",
    "\n",
    "## Reading: Murphy 11.3, 13\n",
    "\n",
    "## 1 Motivating regularization\n",
    "\n",
    "“YEAH, BUT YOUR SCIENTISTS WERE SO PREOCCUPIED WITH WHETHER OR NOT THEY COULD THAT THEY DIDN’T STOP TO THINK IF THEY SHOULD.” -- Dr. Ian Malcolm\n",
    "\n",
    "<img src=\"goldblum.jpg\">\n",
    "\n",
    "\n",
    "We now have the power to create functions (namely neural networks) that have the power to approximate any other function, given a sufficient number of parameters.  However, as we learned when we were fitting polynomials to curves all the way back at the start of the class, unlimited model complexity is a path fraught with peril.  While neural nets are far less prone to overfitting than, say, polynomial regression, there are still good reasons to put priors on the neural network weights.  \n",
    "\n",
    "Today, we will implement two different types of regularization.  First, we will implement so-called L2-regularization, which is equivalent to putting an independent Gaussian prior on each weight.  This is easy to implement: in the training loop, one can simply use the loss function\n",
    "$$\n",
    "\\mathcal{L} = \\text{Data Loss} + \\gamma_1 \\sum (W^{(1)})^2 + \\gamma_2 \\sum (W^{(2)})^2,\n",
    "$$\n",
    "where the summation is over all entries in the weight matrix.  By the way, the bias vectors are typically not regularized: consider why that is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccef571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True)\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "X/=255.\n",
    "y = y.astype(int)\n",
    "X,X_test,y,y_test = train_test_split(X,y,test_size=10000)\n",
    "\n",
    "# Extract number of data points, and the height and width of the images for later reshaping\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "h = 28\n",
    "w = 28\n",
    "\n",
    "N = 10\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212aef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(784,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,10)\n",
    "   \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        # Apply dropout to the input\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.sigmoid(a1)\n",
    "        \n",
    "        # Apply dropout to the hidden layer\n",
    "        a2 = self.l2(z1)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.955814242362976 52.471666666666664 73.47\n",
      "1 1.2767844200134277 64.46333333333334 80.75\n",
      "2 1.1708098649978638 70.35388888888889 82.76\n",
      "3 1.0478030443191528 73.85125 85.08\n",
      "4 0.9162066578865051 76.196 86.13\n",
      "5 1.0053670406341553 77.88194444444444 86.6\n",
      "6 0.8423780202865601 79.15357142857142 87.0\n",
      "7 0.7904657125473022 80.14666666666666 87.3\n",
      "8 0.9380636811256409 80.96685185185186 87.66\n",
      "9 0.6962531805038452 81.64116666666666 88.02\n",
      "10 0.8157628774642944 82.21060606060605 87.9\n",
      "11 0.7824591994285583 82.69930555555555 87.9\n",
      "12 0.6278238296508789 83.13051282051282 88.2\n",
      "13 0.763240396976471 83.51130952380953 88.01\n",
      "14 0.666106641292572 83.84622222222222 88.52\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-1)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "total_train = 0\n",
    "correct_train = 0\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        W1, b1 = [p for p in model.l1.parameters()]\n",
    "        W2, b2 = [p for p in model.l2.parameters()]\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t) + 1e-3*torch.abs(W1).sum()      \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "params = [p for p in model.l1.parameters()]\n",
    "W1_l2 = params[0].cpu().detach().numpy().T\n",
    "\n",
    "fig,axs = plt.subplots(nrows=1,ncols=6)\n",
    "fig.set_size_inches(20,8)\n",
    "for i in range(6):\n",
    "    #axs[0,i].imshow(W1_noreg[:,np.random.randint(W1_noreg.shape[1])].reshape((h,w)))\n",
    "    axs[i].imshow(W1_l2[:,np.random.randint(W1_l2.shape[1])].reshape((h,w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3371c50",
   "metadata": {},
   "source": [
    "Second, we will implement so-called L1-regularization, which is equivalent to putting and independent exponential prior on each weight.  In this case the loss function is\n",
    "$$\n",
    "\\mathcal{L} = \\text{Data Loss} + \\gamma_1 \\sum |W^{(1)}| + \\gamma_2 \\sum |W^{(2)}|.\n",
    "$$\n",
    "Repeat the above, but with this loss.  Despite being similar in both concept and implementation, these two choices yield very different results.  Describe what qualitative characteristics of the weight you think each prior is promoting and why.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-drinking",
   "metadata": {},
   "source": [
    "## More exotic forms of regularization\n",
    "\n",
    "Direct manipulation of weights is perhaps the most obvious way of making the model parameters behave in the way that we would like them to, but they don't often help much on problems of fundamental interest.  Perhaps that's because there's a bit of a jump between \"Make the $L_1$ norm small\" and \"Don't overfit the data.\" More interesting mechanisms for regularization exist that force the model to perform the latter task in somewhat more direct and clever ways.\n",
    "\n",
    "If neural networks can (however tenuously) be thought of as an analog of an animal brain, then overfitting is reasonably well described as memorization.  For the student taking a test, instructors would prefer the student be able to answer a question in a constructive way by understanding the premise of the question being asked and thoughtfully constructing a response.  This is desirable because this mechanism *does not depend upon the specific way in which the question was asked*.  An alternative mechanism that less effective students have been using forever is memorization: if the exact wording of the question (and its answer) are known beforehand, one can simply relate question to answer with no deeper intermediate analysis.  This latter is typically a great strategy for achieving training set accuracy, but very poor for test set accuracy.  \n",
    "\n",
    "In the context of neural networks, these models often have sufficient degrees of freedom to simply memorize inputs.  As an extreme example, if the number of hidden layer nodes is similar to the number of data points, then weights can be adjusted such that exactly one hidden layer node gets activated for exactly one training example, and that hidden layer nodes has non-zero weight to exactly one softmax input.  Thus we have a network that sees a particular pattern of pixels (that it's seen before), and knows that that particular pattern corresponds to (say) a seven.  *Critically, come test time, a correct classification is only possible if the input is (nearly) exactly the same as what the network memorized*.  The useful capacity to weigh the contributions of different features is lost.\n",
    "\n",
    "*Dropout* is a mechanism for ensuring that the above scenario does not occur.  It works as follows: for each training step, we select a random subset of nodes in a layer that we would like to apply dropout to (sometimes a hidden layer, sometimes an input layer, sometimes both), and *turn that node off*, i.e. set its output to zero.  This can be visualized with a classic plot from the [original paper by Srivastava et al.](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) way back in 2014.\n",
    "\n",
    "<img src=\"dropout.png\">\n",
    "Since the node is turned off, its forward mode output is equal to zero, which causes the gradient of any weights directly connected to it to be zero (**Can you show that this is true?**).  *Thus, the neural network is forced to learn to operate without only a random subset of its features available to it at any given time.* If only a random subset of features are available, then the network cannot rely on any given pattern, and thus it cannot simply memorize the training data! (A clever instructor might be able to develop questions that are similarly un-memorizable for their human pupils).  At test time, we don't drop nodes (that would be silly.  Why have them at all then?), but we do scale their outputs by the probability that they were dropped during the training process.  \n",
    "\n",
    "Then dropout is easy to apply in pytorch: all we need to do is to instantiate a dropout layer and apply it after our activation function.  This essentially picks a random subset of the nodes with proportion $p$, and zeros out their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(784,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,10)\n",
    "        \n",
    "        # Instantiate dropout layers\n",
    "        self.dropout_1 = nn.Dropout(p=0.2)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "   \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        # Apply dropout to the input\n",
    "        xd = self.dropout_1(x)\n",
    "        a1 = self.l1(xd)\n",
    "        z1 = torch.sigmoid(a1)\n",
    "        \n",
    "        # Apply dropout to the hidden layer\n",
    "        z1d = self.dropout_2(z1)\n",
    "        a2 = self.l2(z1)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-albert",
   "metadata": {},
   "source": [
    "Here I'm going to do a bit of fancy stuff that we haven't considered yet explicitly (namely, running the model on my GPU and also only processing small chunks of training data at a time, so-called minibatch gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-disco",
   "metadata": {},
   "source": [
    "Now, let's train the model using a similar loop to what you've implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "total_train = 0\n",
    "correct_train = 0\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)      \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "params = [p for p in model.l1.parameters()]\n",
    "W1_dropout = params[0].cpu().detach().numpy().T\n",
    "\n",
    "fig,axs = plt.subplots(nrows=1,ncols=6)\n",
    "fig.set_size_inches(20,8)\n",
    "for i in range(6):\n",
    "    #axs[0,i].imshow(W1_noreg[:,np.random.randint(W1_noreg.shape[1])].reshape((h,w)))\n",
    "    axs[i].imshow(W1_dropout[:,np.random.randint(W1_dropout.shape[1])].reshape((h,w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-matrix",
   "metadata": {},
   "source": [
    "Dropout has worked (in a way that $L_2$ or $L_1$ regularization clearly did not): we get close to 98.3% test set accuracy.\n",
    "\n",
    "# Data Augmentation\n",
    "Dropout is a fantastic tool for encouraging neural networks to not overfit, but it is still suboptimal when compared to the one and only true solution to overfitting: get more data.  This is the one method that always works to justify the inclusion of more complex and powerful models.  The problem of course is that it's often expensive and sometimes impossible.  For example, with the MNIST dataset, we can't just go collect new handwritten digits...  Or can we?\n",
    "\n",
    "Consider the following digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=1,ncols=2)\n",
    "axs[0].imshow(X[5].cpu().reshape((28,28)))\n",
    "\n",
    "# What is being done here?\n",
    "from scipy.ndimage import rotate\n",
    "axs[1].imshow(rotate(X[5].cpu().reshape((28,28)),25,order=1,reshape=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-information",
   "metadata": {},
   "source": [
    "These are both, of course, still the same digit, and that is immediately recognizable to your human visual processing system because somehow it's learned to be *rotationally invariant*.  However, we haven't explicitly taught our neural network that same property: all the rotational invariance it has learned is that which comes from the 60k training examples.  We can force it to learn that property by feeding the network training examples that we have explicitly rotated (probably randomly, and probably within a set range of viable rotations.  **What happens if we train on data that is randomly rotated in the full 360 degrees?**\n",
    "\n",
    "The above process is called *data augmentation*, and is typically done in an *on-line* capacity, which means that as we load mini-batches, we perform some random transformation on each data point, then pass it on to the network as usual.  **Besides rotation, what kinds of transformations can you imagine that we might do on image data?**  We can implement this in pytorch with a custom data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True)\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "X/=255.\n",
    "y = y.astype(int)\n",
    "X,X_test,y,y_test = train_test_split(X,y,test_size=10000)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "### NOTE THAT WE ARE NOT YET PLACING OBJECTS ON GPU ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random transform (20 degree rotation, 3 pixel translation, \n",
    "# scaling between 90% and 110%, and a 10 degree shear over the x-axis) \n",
    "transform = transforms.RandomAffine(20,translate=(0.1,0.1),scale=(0.9,1.1),shear=10)#,transforms.RandomResizedCrop(224),transforms.RandomHorizontalFlip()]\n",
    "\n",
    "# Custom dataset object\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Define a __getitem__ method to be called by the \n",
    "        # mini-batch loader \n",
    "        \n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            # reshape data, apply transform, reflatten\n",
    "            x = x.reshape((28,28))\n",
    "            x = transforms.ToPILImage()(x)\n",
    "            x = self.transform(x)\n",
    "            x = transforms.ToTensor()(x)\n",
    "            x = x.flatten()\n",
    "            #print(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "training_data = CustomTensorDataset([X,y],transform=transform)\n",
    "test_data = CustomTensorDataset([X_test,y_test])\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=16)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False,num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = next(iter(train_loader))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig,axs = plt.subplots(nrows=5,ncols=6)\n",
    "axs = axs.ravel()\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.imshow(t[0][i].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n = 784\n",
    "N = 10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(n,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,N)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.sigmoid(a1)\n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-magic",
   "metadata": {},
   "source": [
    "Unfortunately, this model takes alot longer to train, mostly due to the overhead of augmentation (there are new libraries that allow augmentation to be done quickly and in situ on the GPU in order to help deal with this bottleneck).  We'll load a pre-trained model to help us get to results more quickly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(open('trained_with_augmentation.p','rb'))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "epochs = 5\n",
    "\n",
    "total_train = 0\n",
    "correct_train = 0\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "        # Move batches to GPU\n",
    "        d,t = d.to(device),t.to(device)\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)  \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        d,t = d.to(device),t.to(device)\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-provider",
   "metadata": {},
   "source": [
    "Data augmentation has gotten us to test set accuracies in excess of 98.5%!  This is a remarkable improvement for just jiggling our dataset a bit.  However, it makes sense why it works: the model never trains on the same data point twice, making memorization quite impossible.  Additionally, this property leads to some very compelling looking weight images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters()]\n",
    "W1 = params[0].cpu().detach().numpy().T\n",
    "fig,axs = plt.subplots(nrows=1,ncols=6)\n",
    "fig.set_size_inches(10,4)\n",
    "for i in range(6):\n",
    "    axs[i].imshow(W1[:,np.random.randint(W1.shape[1])].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-germany",
   "metadata": {},
   "source": [
    "No pixel can ever really be relied upon to produce a definitive categorization, so the model has instead learned to recognize broader patterns that are more easily recognizable.  Interestingly, if you know something about classical image processing or numerical partial differential equations, some of the images above will look familiar: asymmetrical patches of positive and negative values are associated with taking the gradient of an image, while a positive patch surrounded by negative patches are analogous to taking the laplacian (essentially the second derivative, or *curvature* of an image).  We've long known that these are nice filters to apply for extracting relevant features from an image; our neural network has just learned these filters on its own!  This notion of *explicitly learning filters* to be applied to images (or any structured data, as we will see) for feature extraction turns out to be a very powerful idea that gives rise to the basis for the current state of the art in image processing (and various tasks in bioinformatics and audio analysis), the *convolutional neural network*, which we will talk about next week.\n",
    "\n",
    "It's also fun to look at the effect of data augmentation on the Labelled Faces in the Wild dataset, as we get a little bit more robust features for facial recognition.  **What is the network looking for, when it can't rely on features being in a particular place?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_model = torch.load(open('lfw_augmented.p','rb'))\n",
    "params = [p for p in lfw_model.parameters()]\n",
    "W1 = params[0].cpu().detach().numpy().T\n",
    "fig,axs = plt.subplots(nrows=1,ncols=6)\n",
    "fig.set_size_inches(10,4)\n",
    "for i in range(6):\n",
    "    axs[i].imshow(W1[:,np.random.randint(W1.shape[1])].reshape((50,37)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-sydney",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
