{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "devoted-native",
   "metadata": {},
   "source": [
    "# Implementing neural networks\n",
    "\n",
    "Today, we're going to go through the process together of implementing a neural network for a simple regression problem.  Then I'm going to turn you loose to adapt this methodology to the MNIST problem.\n",
    "\n",
    "We're going to use a new library to implement this network.  This library is called pytorch, and you can easily install it by following the installation instructions found [here](https://pytorch.org/get-started/locally/).  Why are we not using numpy?  We'll return to that in a moment.  However, torch actually behaves significantly like numpy in a variety of ways.  For example we can generate a synthetic dataset (rather similar to that from Homework 2) using some familiar commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suited-robert",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the features\n",
    "x = torch.linspace(-2,2,101).reshape(-1,1)\n",
    "\n",
    "# Generate the response variables \n",
    "y_obs = x**2 + x + torch.cos(2*np.pi*x) + torch.randn_like(x)*0.3\n",
    "\n",
    "plt.plot(x,y_obs,'k.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-hanging",
   "metadata": {},
   "source": [
    "We'd like to find a function that fits this data.  One way to do this is, of course, linear regression, but that requires specifying the form of the design matrix.  As we saw in lecture, we'd like to learn the design matrix from the data.  We'll use a neural network to perform this task.  Algebraically, we can specify the neural network via the following sequentially applied functions.\n",
    "$$\n",
    "\\underbrace{z}_{m\\times p} = \\underbrace{x}_{m\\times 1} \\underbrace{W^{(1)}}_{1\\times p} + \\underbrace{b^{(1)}}_{1\\times p}\n",
    "$$\n",
    "$$\n",
    "h = \\sigma(z)\n",
    "$$\n",
    "$$\n",
    "\\underbrace{y}_{m\\times 1} = h \\underbrace{W^{(2)}}{p \\times 1} + \\underbrace{b^{(2)}}_{1\\times 1}\n",
    "$$\n",
    "\n",
    "Because this is getting a bit complicated, let's develop a create a class for this neural network to hold weights and apply functions as appropriate.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \n",
    "    def __init__(self,n,p,N):\n",
    "        self.n = n   # Number of features (1 for univariate problem)\n",
    "        self.p = p   # Number of nodes in the hidden layer\n",
    "        self.N = N   # Number of outputs (1 for the regression problem)\n",
    "        \n",
    "        # Instantiate weight matrices \n",
    "        self.W_1 = torch.randn(n,p)*10\n",
    "        self.W_2 = torch.randn(p,N)/np.sqrt(p)*1\n",
    "        \n",
    "        # Instantiate bias vectors (Why do we need this?)\n",
    "        self.b_1 = torch.randn(1,p)*10\n",
    "        self.b_2 = torch.randn(1,N)/np.sqrt(p)*1\n",
    "               \n",
    "    def forward(self,X):\n",
    "        # Applies the neural network model\n",
    "        ## All of these self. prefixes save calculation results\n",
    "        ## as class variables - we can inspect them later if we\n",
    "        ## wish to\n",
    "        self.X = X\n",
    "        self.z = self.X @ self.W_1 + self.b_1  # First linear \n",
    "        self.h = torch.sigmoid(self.z)         # Activation\n",
    "        self.y = self.h @ self.W_2 + self.b_2  # Second linear\n",
    "        \n",
    "        return self.y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-saint",
   "metadata": {},
   "source": [
    "You'll notice that we're instantiating weights randomly.  Let's see what type of functions this model produces, prior to training.  It's interesting to see what the effect is of messing with the variance of the weights when initializing them.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10 random neural nets\n",
    "for i in range(10):\n",
    "    \n",
    "    # Create the neural network\n",
    "    net = NeuralNet(1,20,1)\n",
    "    \n",
    "    # Make a prediction\n",
    "    y_pred = net.forward(x)\n",
    "    \n",
    "    # Plot the predictions\n",
    "    plt.plot(x,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-depth",
   "metadata": {},
   "source": [
    "Of course, this isn't all that interesting on its own.  We now need to train this thing.  We'll do this using gradient descent, and herein lies the power of pytorch.  It is a framework for *automatic differentiation*.  What does this mean?  It means that it keeps a record of all of the operations that have been done to produce the output of a given function.  It then can *automaticall* apply the chain rule to produce derivatives of a function with respect to anything that was used to compute it.  Here, we're hoping to take the gradient with respect to the weights and biases.  We can tell pytorch that we're going to want these things by using the \"requires_grad_\" flag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \n",
    "    def __init__(self,n,p,N):\n",
    "        self.n = n   # Number of features (1 for univariate problem)\n",
    "        self.p = p   # Number of nodes in the hidden layer\n",
    "        self.N = N   # Number of outputs (1 for the regression problem)\n",
    "        \n",
    "        # Instantiate weight matrices \n",
    "        self.W_1 = torch.randn(n,p)*10\n",
    "        self.W_2 = torch.randn(p,N)/np.sqrt(p)\n",
    "        \n",
    "        # Instantiate bias vectors (Why do we need this?)\n",
    "        self.b_1 = torch.randn(1,p)*10\n",
    "        self.b_2 = torch.randn(1,N)/np.sqrt(p)\n",
    "        \n",
    "        ### CHANGE FROM ABOVE ###  \n",
    "        # Collect the model parameters, and tell pytorch to\n",
    "        # collect gradient information about them.\n",
    "        self.parameters = [self.W_1,self.W_2,self.b_1,self.b_2]\n",
    "        for param in self.parameters:\n",
    "            param.requires_grad_()\n",
    "    def forward(self,X):\n",
    "        # Applies the neural network model\n",
    "        ## All of these self. prefixes save calculation results\n",
    "        ## as class variables - we can inspect them later if we\n",
    "        ## wish to\n",
    "        self.X = X\n",
    "        self.z = self.X @ self.W_1 + self.b_1  # First linear \n",
    "        self.h = torch.sigmoid(self.z)         # Activation\n",
    "        self.y = self.h @ self.W_2 + self.b_2  # Second linear\n",
    "        return self.y\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        ### Each parameter has an additional array associated\n",
    "        ### with it to store its gradient.  This is not \n",
    "        ### automatically cleared, so we have a method to\n",
    "        ### clear it.\n",
    "        for param in self.parameters:\n",
    "            try:\n",
    "                param.grad.data[:] = 0.0\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-petersburg",
   "metadata": {},
   "source": [
    "One thing that still need is something to minimize.  Since this is a regression problem, we'll use mean-squared-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred,y_obs):\n",
    "    m = y_pred.shape[0]\n",
    "    return 1./m*((y_pred-y_obs)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-harris",
   "metadata": {},
   "source": [
    "Now, the code for gradient descent becomes strikingly simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(1,20,1)  # Instantiate network\n",
    "eta = 1e-1               # Set learning rate (empirically derived)\n",
    "for t in range(50000):   # run for 50000 epochs\n",
    "    y_pred = net.forward(x)   # Make a prediction\n",
    "    L = mse(y_pred,y_obs)     # Compute mse\n",
    "    net.zero_grad()           # Clear gradient buffer\n",
    "    L.backward()              # MAGIC: compute dL/d parameter\n",
    "    for param in net.parameters:            # update parameters w/\n",
    "        param.data -= eta*param.grad.data   # GD\n",
    "        \n",
    "    if t%100==0:         # Print loss    \n",
    "        print(t,L.item())\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-relation",
   "metadata": {},
   "source": [
    "Now we can plot our model prediction versus observations.  Pretty good!  And no manual selection of basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.detach().squeeze(),y_pred.detach().squeeze())\n",
    "plt.plot(x.detach().squeeze(),y_obs.detach().squeeze(),'k.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-alliance",
   "metadata": {},
   "source": [
    "## Applying an MLP to MNIST:\n",
    "Train a neural on MNIST using pytorch.  You should use the above code as a template.  Things you'll need to change: $n$ will no longer be 1, but rather 784.  $N$ will no longer be one, but 10.  You'll want to adjust $p$, the number of hidden layer nodes.  You'll likely need to adjust the learning rate.  Finally, and most importantly, you'll need to use a different loss function.  In particular, you'll replace our handrolled MSE code with [this](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).  IMPORTANT NOTE: this loss expects *logits* as inputs, which is to say that it will do softmax for you internally.  As such, the architecture of your network should be more or less the same as above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "X = X[:5000]\n",
    "y = y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= X.mean()\n",
    "X /= X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet(object):\n",
    "    \n",
    "    def __init__(self,n,p,N):\n",
    "        self.n = n   # Number of features (1 for univariate problem)\n",
    "        self.p = p   # Number of nodes in the hidden layer\n",
    "        self.N = N   # Number of outputs (1 for the regression problem)\n",
    "        \n",
    "        # Instantiate weight matrices \n",
    "        self.W_1 = torch.randn(n,p)*1e-3\n",
    "        self.W_2 = torch.randn(p,N)*1e-3\n",
    "        \n",
    "        # Instantiate bias vectors (Why do we need this?)\n",
    "        self.b_1 = torch.randn(1,p)\n",
    "        self.b_2 = torch.randn(1,N)\n",
    "        \n",
    "        ### CHANGE FROM ABOVE ###  \n",
    "        # Collect the model parameters, and tell pytorch to\n",
    "        # collect gradient information about them.\n",
    "        self.parameters = [self.W_1,self.W_2,self.b_1,self.b_2]\n",
    "        for param in self.parameters:\n",
    "            param.requires_grad_()\n",
    "    def forward(self,X):\n",
    "        # Applies the neural network model\n",
    "        ## All of these self. prefixes save calculation results\n",
    "        ## as class variables - we can inspect them later if we\n",
    "        ## wish to\n",
    "        self.X = X\n",
    "        self.z = self.X @ self.W_1 + self.b_1  # First linear \n",
    "        self.h = torch.sigmoid(self.z)         # Activation\n",
    "        self.y = self.h @ self.W_2 + self.b_2  # Second linear\n",
    "        return self.y\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        ### Each parameter has an additional array associated\n",
    "        ### with it to store its gradient.  This is not \n",
    "        ### automatically cleared, so we have a method to\n",
    "        ### clear it.\n",
    "        for param in self.parameters:\n",
    "            try:\n",
    "                param.grad.data[:] = 0.0\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_train = torch.from_numpy(X).to(torch.float)\n",
    "y_train = torch.from_numpy(y.astype(int)).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(784,200,10)  # Instantiate network\n",
    "eta = 1e-2               # Set learning rate (empirically derived)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "for t in range(50000):   # run for 50000 epochs\n",
    "    y_pred = net.forward(X_train)   # Make a prediction\n",
    "    L = loss(y_pred,y_train)     # Compute mse\n",
    "    net.zero_grad()           # Clear gradient buffer\n",
    "    L.backward()              # MAGIC: compute dL/d parameter\n",
    "    for param in net.parameters:            # update parameters w/\n",
    "        param.data -= eta*param.grad.data   # GD\n",
    "        \n",
    "    if t%10==0:         # Print loss    \n",
    "        print(t,L.item(),(torch.argmax(y_pred,axis=1)==y_train).sum()/len(y_train))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(net.W_1[:,6].reshape(28,28).detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
