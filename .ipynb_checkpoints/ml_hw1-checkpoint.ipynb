{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/esther-lyondelsordo/CSCI-447-Machine-Learning/blob/main/ml_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CSCI 447 Machine Learning Homework 1</h1>\n",
    "Esther Lyon Delsordo\n",
    "<br>\n",
    "Due: 9/22/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuXhGEdGWWgQ"
   },
   "source": [
    "<h1> Problem 1: Legal Reasoning </h1>\n",
    "Suppose a crime has been committed. Blood is found at the scene for which there is no innocent explanation. It is of a type which is present in 1% of the population.\n",
    "\n",
    "<h3> 1. prosecutor's fallacy </h3> \n",
    "The prosecutor claims: “There is a 1% chance that the defendant would have the crime blood type if he were innocent. Thus there is a 99% chance that he guilty”. This is known as the prosecutor’s fallacy. What is wrong with this argument?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe8dh9lu9ZTy"
   },
   "source": [
    "**Ans:** The prosecutor is assuming that $ P(Guilty | Blood Type) = P(Blood Type | Guilty) $. According to Bayes Theorem, this cannot be true. By Bayes, the correct relationship is: $ P(Guilty | Blood Type) = \\frac{P(Blood Type | Guilty)P(Guilty)}{P(Blood Type)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fanvzqnh9cN5"
   },
   "source": [
    "<h3> 2. defender's fallacy </h3>\n",
    "The defender claims: “The crime occurred in a city of 800,000 people. The blood type would be found in approximately 8000 people. The evidence has provided a probability of just 1 in 8000 that the defendant is guilty, and thus has no relevance.” This is known as the defender’s fallacy. What is wrong with this argument?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOXStCco9cy-"
   },
   "source": [
    "**Ans:** The defender is claiming that $ P(Guilty | Blood Type) = P(Blood Type) $. As we can see from the Bayes Theorem representation above, this logic is not sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPcZ5d81Wk1o"
   },
   "source": [
    "<h1> Problem 2: Poisson Distribution MLE </h1>\n",
    "The Poisson distribution is a useful model when we would like to model a count as a random variable. For\n",
    "example, the number of letters a person gets in a day is an example of something that could be Poisson distributed (other examples: the number of deer that appear on a game camera in a day, the number of\n",
    "meteorites to strike the earth in a year). The probability mass function of a Poisson distribution is\n",
    "$$ P(X = k) = \\frac{λ^ke^{-λ}}{k!}$$\n",
    "where k is the number of times an event occurs, and λ is the rate parameter. Given some dataset\n",
    "X1, X2, . . . , Xm, derive the maximum likelihood estimator for λ. Evaluate your estimator for the data\n",
    "set [1, 2, 2, 3]. (HINT: the procedure here is just like for the Bernoulli distribution: logarithm, derivative, set\n",
    "to zero, solve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SOUNd8NWw-V"
   },
   "source": [
    "**Ans:** The relationship above can be written \n",
    "$$P(X = k | \\lambda) = \\frac{λ^ke^{-λ}}{k!}$$\n",
    "If we are finding the probability of a vector $\\vec X$ being equal to the data, $\\vec k$, both of length $m$, then we have:\n",
    "$$P(\\vec X = \\vec k | \\lambda) = \\prod ^m _{i=1} \\frac{λ^{k_i}e^{-λ}}{k_i!}$$\n",
    "We can compute the log likelihood to make the math nicer:\n",
    "$$L(\\lambda) = ln( P(\\vec X = \\vec k | \\lambda) ) = \\sum ^m _{i=1} k_i ln(k_i) - \\lambda - ln(k_i!)$$\n",
    "To maximize $\\lambda$, we want to take the derivative of $L$ and set it equal to zero:\n",
    "$$\\frac{\\partial L}{\\partial \\lambda} = \\sum ^m _{i=1} \\frac{k_i}{\\lambda}-1 = 0$$\n",
    "$$\\sum ^m _{i=1} \\frac{k_i}{\\lambda} = \\sum ^m _{i=1} 1$$\n",
    "$$\\sum ^m _{i=1} \\frac{k_i}{\\lambda} = m$$\n",
    "And so we get the Maximum Likelihood Estimator for the Poisson Distribution:\n",
    "$$\\sum ^m _{i=1} \\frac{k_i}{m} = \\lambda$$\n",
    "With our data we have $m=4$, so:\n",
    "$$\\lambda_{data} = \\frac{1}{4} + \\frac{2}{4} + \\frac{2}{4} + \\frac{3}{4} = \\frac{8}{4} = 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfpooLNIWx6Q"
   },
   "source": [
    "<h1> Problem 3: Naive Bayes </h1>\n",
    "When utilizing naive Bayes, we have relied upon maximum likelihood estimation to estimate the parameters of categorical distributions over words conditioned on a class, i.e.\n",
    "$$ P(X = x|C = c) $$ \n",
    "for x in a vocabulary and c in a set of classes. Describe what happens if, having adopted this strategy,\n",
    "we attempt to apply naive Bayes to a message that contains a word that did not appear in the training\n",
    "corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16JfS-wiAJht"
   },
   "source": [
    "**Ans:** We wouldn't have any information about the experimental probability of the word belonging to a certain class. This would mean that the prior and the model would be innacurate for this data point. For naive Bayes, we can say that:\n",
    "$$P(X=x|C=c) = \\frac{m_{x,c}}{m_c} $$\n",
    "Where $m_{x,c}$ is the number of times the word $x$ appears in in class $c$ of the vocabulary, and $m_c$ is the total number of words in class $c$. The number of words in class $c$, $m_c$, could not account for the word $x$ because this word is not in the vocabulary, giving us $m_c =0$, and dividing by zero can cause problems. We would also end up with $m_{x,c}=0$ because the word would not appear in any class of the vocabulary if it isn't there at all. We are given a probability of zero for that word in the message. And seeing as the probability of a message occuring is a product:\n",
    "$$P(C=c|X=x) \\propto P(C=c) \\prod_{i=1}^n P(X_i=x_i|C=c) $$\n",
    "The zero probability for one word would then result in a zero probability for the entire message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJTB2el+pRhCcxUXcwWbIQ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
