{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8971f97c",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "Esther Lyon Delsordo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33751c1",
   "metadata": {},
   "source": [
    "## Problem 1: Overparameterization\n",
    "Consider a multi-class classification problem (N = 3) with a single feature (called x). Assuming that the\n",
    "logits are related to the feature via a linear model, we can write the conditional class probabilities as\n",
    "$$\n",
    "P(y = k|x) = \\frac{exp(W_{0,k} + W_{1,k}x)}{\\Sigma_j^N exp (W_{0,j} + W_{1,j}x)}\n",
    "$$\n",
    "This model has 2N free parameters, two for each class. Show that this model is overparameterized; specifically, show that this model can be equivalently represented with 2(N − 1) free parameters. HINT: With two\n",
    "classes, this procedure exactly recovers logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3c8b260",
   "metadata": {},
   "source": [
    "## Problem 2: Backpropagation\n",
    "A\n",
    "Derive and evaluate an expression for\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_2^{(2)}}\n",
    "$$\n",
    "Create a python script that computes this derivative for the feature x = 1.0, data y = 0.5, and parameter\n",
    "values\n",
    "$$\n",
    "w^{(1)}_1 = 0.5 \n",
    "\\\\ w^{(2)}_1 = 0.7 \n",
    "\\\\ w^{(2)}_2 = −0.3\n",
    "\\\\ w^{(3)}_1 = 0.1 \n",
    "\\\\ w^{(3)}_2 = −0.8\n",
    "$$\n",
    "As a check, the correct value is approximately 0.099."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74dbf31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16e8685",
   "metadata": {},
   "source": [
    "## Problem 3: Robust Cross-Entropy\n",
    "Consider a binary classification problem in which the target values are $yobs \\in 0, 1$, with a network output\n",
    "y(x, w) that represents P(y = 1|x) (i.e. logistic regression). Now suppose that there is a probability $\\epsilon$ that the class label on a training data point has been incorrectly set. Assuming independent and identically\n",
    "distributed data, write down the negative log likelihood. Verify that the cross-entropy error function (Murphy\n",
    "10.27) is obtained when \u000f = 0. Note that this error function makes the model robust to incorrectly labelled\n",
    "data, in contrast to the usual error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a02d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
