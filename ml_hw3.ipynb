{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8971f97c",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "Esther Lyon Delsordo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33751c1",
   "metadata": {},
   "source": [
    "## Problem 1: Overparameterization\n",
    "Consider a multi-class classification problem (N = 3) with a single feature (called x). Assuming that the\n",
    "logits are related to the feature via a linear model, we can write the conditional class probabilities as\n",
    "$$\n",
    "P(y = k|x) = \\frac{exp(W_{0,k} + W_{1,k}x)}{\\Sigma_j^N exp (W_{0,j} + W_{1,j}x)}\n",
    "$$\n",
    "This model has 2N free parameters, two for each class. Show that this model is overparameterized; specifically, show that this model can be equivalently represented with 2(N − 1) free parameters. HINT: With two\n",
    "classes, this procedure exactly recovers logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9767ffb",
   "metadata": {},
   "source": [
    "**Solution:** Consider the case with only $N = 3$ classes. We can write the conditional class probabilities for $y=1$ as:\n",
    "$$\n",
    "P(y = 1|x) = \\frac{exp(W_{0,1}+W_{1,1}x)}{exp(W_{0,1}+W_{1,1}x)+exp(W_{0,2}+W_{1,2}x)+exp(W_{0,3}+W_{1,3}x)}\n",
    "$$\n",
    "Then if we multiply by a \"funky one\" (an identity), we get:\n",
    "$$\n",
    "\\frac{\\frac{1}{exp(W_{0,1}+W_{1,1}x)}}{\\frac{1}{exp(W_{0,1}+W_{1,1}x)}} \\times \\frac{exp(W_{0,1}+W_{1,1}x)}{exp(W_{0,1}+W_{1,1}x)+exp(W_{0,2}+W_{1,2}x)+exp(W_{0,3}+W_{1,3}x)}\n",
    "\\\\\n",
    "= \\frac{1}{1+\\frac{exp(W_{0,2}+W_{1,2}x)}{exp(W_{0,1}+W_{1,1}x)}+\\frac{exp(W_{0,3}+W_{1,3}x)}{exp(W_{0,1}+W_{1,1}x)}}\n",
    "\\\\\n",
    "= \\frac{1}{1+exp(W_{0,2}+W_{1,2}x-W_{0,1}-W_{1,1}x)+exp(W_{0,3}+W_{1,3}x-W_{0,1}-W_{1,1}x)}\n",
    "$$\n",
    "Now, let's redefine our variables by combining like terms. Let:\n",
    "$$\n",
    "b_1 = W_{0,2}-W_{0,1}\n",
    "\\\\\n",
    "m_1 = W_{1,2}-W{1,1}\n",
    "\\\\\n",
    "b_2 = W_{0,3}-W_{0,1}\n",
    "\\\\\n",
    "m_2 = W_{1,3}-W{1,1}\n",
    "$$\n",
    "We get:\n",
    "$$\n",
    "P(y = 1|x) = \\frac{1}{1+exp(b_1+m_1x)+exp(b_2+m_2x)}\n",
    "$$\n",
    "This version only has $4 = 2(3-1)$ parameters, showing that our original model was overparametrized. Further, by taking the $N=2$ class version, we see that we have a logistic regression with:\n",
    "$$\n",
    "P(y = 1|x) = \\frac{1}{1+exp(W_{0,2}+W_{1,2}x-W_{0,1}-W_{1,1}x)}\n",
    "\\\\ = \\frac{1}{1 + exp(b_1+m_1x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8b260",
   "metadata": {},
   "source": [
    "## Problem 2: Backpropagation\n",
    "### A\n",
    "Derive and evaluate an expression for\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_2^{(2)}}\n",
    "$$\n",
    "Create a python script that computes this derivative for the feature x = 1.0, data y = 0.5, and parameter\n",
    "values\n",
    "$$\n",
    "w^{(1)}_1 = 0.5 \n",
    "\\\\ w^{(2)}_1 = 0.7 \n",
    "\\\\ w^{(2)}_2 = −0.3\n",
    "\\\\ w^{(3)}_1 = 0.1 \n",
    "\\\\ w^{(3)}_2 = −0.8\n",
    "$$\n",
    "As a check, the correct value is approximately 0.099."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeed0ae",
   "metadata": {},
   "source": [
    "If we use the property $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$ to compute $\\frac{\\partial z_2^{2}}{\\partial a_2^{(2)}}$, we get:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial w_2^{(2)}} &=& \n",
    "\\frac{\\partial L}{\\partial z^{(3)}}\\frac{\\partial z^{(3)}}{\\partial a^{(3)}}\\frac{\\partial a^{(3)}}{\\partial z_2^{(2)}}\\frac{\\partial z_2^{(2)}}{\\partial a_2^{(2)}}\\frac{\\partial a_2^{(2)}}{\\partial w_2^{(2)}}\n",
    "\\\\\n",
    "\\\\\n",
    "&=& (z^{(3)}-y)(1)(w_2^{(3)})(\\sigma(a_2^{(2)})(1-\\sigma(a_2^{(2)})))(z^{(1)})\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74dbf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09898163577188604"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given values\n",
    "w1_1 = 0.5\n",
    "w2_1 = 0.7\n",
    "w2_2 = -0.3\n",
    "w3_1 = 0.1\n",
    "w3_2 = -0.8\n",
    "x = 1.\n",
    "y = 0.5\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1./(1 + np.exp(-z))\n",
    "\n",
    "# Calculate other variables from graph equations\n",
    "a1 = x*w1_1\n",
    "z1 = sigmoid(a1)\n",
    "a2_1 = z1*w2_1\n",
    "a2_2 = z1*w2_2\n",
    "z2_2 = sigmoid(a2_2)\n",
    "z2_1 = sigmoid(a2_1)\n",
    "a3 = z2_1*w3_1+z2_2*w3_2\n",
    "z3 = a3\n",
    "\n",
    "# Calculate the derivative\n",
    "dLdw2_2 = (z3-y)*1*w3_2*(sigmoid(a2_2)*(1-sigmoid(a2_2)))*z1\n",
    "dLdw2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e8685",
   "metadata": {},
   "source": [
    "## Problem 3: Robust Cross-Entropy\n",
    "Consider a binary classification problem in which the target values are $y_{obs} \\in 0, 1$, with a network output\n",
    "y(x, w) that represents P(y = 1|x) (i.e. logistic regression). Now suppose that there is a probability $\\epsilon$ that the class label on a training data point has been incorrectly set. Assuming independent and identically\n",
    "distributed data, write down the negative log likelihood. Verify that the cross-entropy error function (Murphy\n",
    "10.27) is obtained when $\\epsilon$ = 0. Note that this error function makes the model robust to incorrectly labelled\n",
    "data, in contrast to the usual error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a02d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
