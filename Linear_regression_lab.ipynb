{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cricket.jpg\" width=30%>\n",
    "For centuries, it's been understood that the frequency of cricket chirps increases as temperature increases.  In this problem, you will determine the functional relationship between these two variables such that cricket chirps can be used as a thermometer. \n",
    "\n",
    "To begin, import the data file cricket.txt.  The first column is the temperature in degrees C, while the second column is the number of cricket chirps per 15 seconds.  Using scikit-learn's model selection tools, we can split the data into a training set, which will be used to train the model, and a test set, which will be used to validate the model's performance on data that was *not* used to train it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt('crickets.txt')\n",
    "\n",
    "data -= data.mean(axis=0)\n",
    "data /= data.std(axis=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[:,0], data[:,1], test_size=0.5, random_state=42)\n",
    "\n",
    "train_idx = np.argsort(X_train)\n",
    "X_train = X_train[train_idx]\n",
    "Y_train = Y_train[train_idx]\n",
    "\n",
    "\n",
    "test_idx = np.argsort(X_test)\n",
    "X_test = X_test[test_idx]\n",
    "Y_test = Y_test[test_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X_train,Y_train,'ko')\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('Freq.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X,Y into Z\n",
    "Z_train = np.c_[X_train,Y_train]\n",
    "\n",
    "mu_z = Z_train.mean(axis=0)\n",
    "Sigma_z = np.cov(Z_train.T)\n",
    "\n",
    "mu_x = mu_z[0]\n",
    "mu_y = mu_z[1]\n",
    "\n",
    "sigma2_xy = Sigma_z[0,1]\n",
    "sigma2_x = Sigma_z[0,0]\n",
    "sigma2_y = Sigma_z[1,1]\n",
    "\n",
    "mu_bar = mu_y + sigma2_xy/sigma2_x*(X_train-mu_x)\n",
    "sigma2_bar = sigma2_y - sigma2_xy**2/sigma2_x \n",
    "\n",
    "\n",
    "plt.plot(X_train,Y_train,'ko')\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('Freq.')\n",
    "plt.errorbar(X_train,mu_bar,yerr=2*np.sqrt(sigma2_bar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ordinary Least Squares\n",
    "Your first task is to define a function that will fit a polynomial of arbitrary degree to the data, subject to Tikhonov regularization.  To do this you will have to generate the Design matrix $\\Phi(X_{obs})$, and solve the normal equations \n",
    "$$\n",
    "(\\Phi^T \\Phi + \\lambda I) \\mathbf{w} = \\phi^T Y_{obs},\n",
    "$$\n",
    "where $\\mathbf{w}$ is the vector of polynomial coefficients.  Plot the data with the best-fitting polynomial of degree 1 (a line) overlain.  A handy fact is that if you would like to evaluate this model at some location (or set of locations) $X_{pred}$, the corresponding *prediction* $Y_{pred}$ is given by \n",
    "$$\n",
    "Y_{pred} = \\underbrace{\\Phi(X_{pred})}_{m\\times n} \\underbrace{\\mathbf{w}}_{n\\times 1}.\n",
    "$$\n",
    "As such, it might be helpful to define a function that computes $\\Phi(X)$ outside of fit\\_polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial(X,Y,d,lamda=0):\n",
    "    \"\"\"  Find the ordinary least squares fit of an independent \n",
    "        variable X to a dependent variable y\"\"\"\n",
    "    Phi = np.column_stack([X**i for i in range(d+1)])\n",
    "    mod_I = np.eye(d+1)\n",
    "    #mod_I[0,0] = 0.0\n",
    "    w = np.linalg.solve(Phi.T @ Phi + lamda*mod_I,Phi.T @ Y)\n",
    "      \n",
    "    return w, Phi\n",
    "\n",
    "w_line, Phi_train = fit_polynomial(X_train,Y_train,100,lamda=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_interp = np.linspace(-2,2,1000)\n",
    "d = 100\n",
    "Phi_interp = np.column_stack([X_interp**i for i in range(d+1)])\n",
    "Y_interp = Phi_interp @ w_line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_interp,Y_interp)\n",
    "plt.plot(X_train,Y_train,'ko')\n",
    "plt.ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Overfitting\n",
    "With the above function in hand, now we will explore the effect of fitting higher degree polynomials to the data.  Fit the training data using polynomials from degree 1 to 15, without regularization (i.e. $\\lambda=0$).  For each of these fits, record the resulting root mean square error \n",
    "$$\n",
    "RMSE = \\sqrt{\\sum_{i=1}^m (Y_{pred,i} - Y_{obs,i})^2}\n",
    "$$\n",
    "\n",
    "in both the training and test data.  Plot both of these RMSE values as a function of polynomial degree (Using a logarithmic scale for RMSE is helpful). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse_list = []\n",
    "test_rmse_list = []\n",
    "m = len(Y_train)\n",
    "m_test = len(Y_test)\n",
    "degrees = np.linspace(0,15,16).astype(int)\n",
    "for d in degrees:\n",
    "    w_line, Phi_train = fit_polynomial(X_train,Y_train,d,lamda=0)\n",
    "    Y_train_pred = Phi_train @ w_line\n",
    "    rmse_train = np.sqrt(((Y_train_pred - Y_train)**2).sum()/m)\n",
    "    train_rmse_list.append(rmse_train)\n",
    "    \n",
    "    _, Phi_test = fit_polynomial(X_test,Y_test,d,lamda=0)\n",
    "    Y_test_pred = Phi_test @ w_line\n",
    "    rmse_test = np.sqrt(((Y_test_pred - Y_test)**2).sum()/m_test)\n",
    "    test_rmse_list.append(rmse_test)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #! Use the function you generated above to fit \n",
    "    #! a polynomial of degree d to the cricket data\n",
    " \n",
    "    #! Compute and record RMSE for both the training and\n",
    "    #! test sets.  IMPORTANT: Don't fit a new set of \n",
    "    #! weights to the test set!!!\n",
    "\n",
    "plt.semilogy(degrees,train_rmse_list,'-')\n",
    "plt.semilogy(degrees,test_rmse_list,'--')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regularization\n",
    "Fix the polynomial degree at 15, and now fit the training data for regularization parameter $\\lambda \\in [10^{-9},10^2]$ (you'll want to distribute these points in log-space; see below).  As above, compute the RMSE in the training and test sets, and plot as a function of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = []\n",
    "test_rmse = []\n",
    "lamdas = np.logspace(-9,2,12)\n",
    "d = 15\n",
    "for lamda in lamdas:\n",
    "    #! Use the function you generated above to fit \n",
    "    #! a polynomial of degree 15 to the cricket data\n",
    "    #! with varying lambda \n",
    "    \n",
    "    #! Compute and record RMSE for both the training and\n",
    "    #! test sets.  IMPORTANT: Don't fit a new set of \n",
    "    #! weights to the test set!!!\n",
    "\n",
    "#plt.loglog(lamdas,train_rmse)\n",
    "#plt.loglog(lamdas,test_rmse)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mauna Kea CO2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "co2 = fetch_openml(data_id=41187, as_frame=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "co2_data = co2.frame\n",
    "co2_data[\"date\"] = pd.to_datetime(co2_data[[\"year\", \"month\", \"day\"]])\n",
    "co2_data = co2_data[[\"date\", \"co2\"]].set_index(\"date\")\n",
    "co2_data.head()\n",
    "\n",
    "Y = co2_data['co2'].to_numpy()\n",
    "X = np.linspace(0,43,len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X,Y)\n",
    "\n",
    "\n",
    "Phi = np.c_[np.ones_like(X),X,np.sin(2*np.pi*X)]\n",
    "w_opt = np.linalg.solve(Phi.T @ Phi,Phi.T @ Y)\n",
    "\n",
    "\n",
    "plt.plot(X,Phi @ w_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
